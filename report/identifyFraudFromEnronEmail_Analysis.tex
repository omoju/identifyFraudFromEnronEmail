\documentclass[twoside,openright,titlepage,numbers=noenddot,headinclude,%
               footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,%
               paper=a4,fontsize=11pt,ngerman,american]{scrreprt}

% Custom config ===============================================================

% Classic thesis
\usepackage{amssymb}
\input{classicthesis-config}

% Theorems and definitions
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\newtheorem{algorithm}{Algorithm}
\usepackage{algpseudocode}

% Counters
\renewcommand{\labelenumi}{{\color{halfgray}(\alph{enumi})}}
\renewcommand{\labelenumii}{\color{halfgray}{\roman{enumii}.}}
\renewcommand{\labelitemi}{{\color{halfgray}-}}%\raisebox{0.3ex}{\tiny$\blacksquare$}}}

\numberwithin{theorem}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{algorithm}{chapter}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}

% Maths
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\numberwithin{equation}{chapter}
\allowdisplaybreaks

% Shaded boxes
\usepackage{framed}
\newenvironment{remark}[1]{%
  \definecolor{shadecolor}{gray}{0.9}%
  \begin{shaded}{\color{Maroon}\noindent\textsc{#1}}\\%
}{%
  \end{shaded}%
}



% Code snippets
\usepackage{color}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}


% Todo
\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

% PS pictures
%\usepackage{pstricks,auto-pst-pdf}

% Landscape tables
\usepackage{rotating}

% Checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Wide tables
\usepackage{ltablex}

%highlighting
\usepackage{color}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}


% -----------------------------------------------------------------------------

\begin{document}
\frenchspacing
\raggedbottom
\selectlanguage{american}
\pagenumbering{roman}
\pagestyle{plain}

% Front pages =================================================================

\include{titlepage}

% Content =====================================================================

\pagenumbering{arabic}

\cleardoublepage


\chapter*{Identify Fraud From Enron Data: Introduction}

%----------------------------------------------------------------------------------------
%  CHAPTER CONTENTS
%----------------------------------------------------------------------------------------


\section*{Project Overview}

In early 2000s, the Enron corporation of Houston, Texas was considered one of the most profitable energy companies in the United States. Fortune magazine named Enron one of the most innovative companies in the US for six years in a row, 1996 - 2001. However, by late 2001, the Enron corporation filed for bankruptcy after it was revealed that the company overstated its profits and defrauded it share holders of a considerable amount of wealth. Several person were later indicted and convicted of fraud in the Enron case. 

The goal of this project is to use applied machine learning, based on released Enron data---Enron emails and financial accounts---to identify persons of interest in the Enron debacle.


\section*{Problem Statement}

The problem that we are interested in solving is building an algorithm that can help us identify persons who might be of interest with regards to fraud at Enron. These persons are predominantly Enron employees and consultants who worked for the corporation. 

The dataset that we have available for this task was collected over two weeks in 2002 by the Federal Energy Regulatory Commission (FERC) during its investigation into the case. The dataset contains over 600,000 emails of 158 high-level Enron employees as well as the financial records of stock payments, salary and so forth of those employees.

To identify persons of interest that might have been party to fraud at Enron we suggest the following strategy:
\begin{enumerate}% 
\item Explore the dataset to ensure its integrity and understand the context.
\item Identify features that may be used. If possible, engineer features that might provide greater discrimination.
\item With the understanding that this a ``classification'' task, explore a couple of classifiers that might be well suited for the problem at hand.
\item Once classifiers have been identified, tune them for optimality.
\end{enumerate}



\section*{Metrics}

The Enron dataset while it contains a robust amount of emails, contains data for only about 150 people. An interesting aspect of the Enron data is that the class labels for our classification is heavily unbalanced at a ratio of around 6:1 in favor of negative examples as can be seen in figure \ref{poi}. 

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/poi}
    \label{poi}}
    
    \caption{\textbf{Person of Interest plot.}\textit{ By visualizing this feature which acts as the label for our classifiers, we can see that the dataset is extremely skewed in favor of non-persons of interest.}}
\end{figure}


For this reason, using ``Accuracy'' as a performance metric leads to misleading information. A more apt measure of the performance of a learner should take into account the results of a confusion matrix and  calculate ``precision'' and ``recall,'' noting that precision is more a measure of a classifiers exactness.


%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Identify Fraud From Enron Data: Analysis}

\section*{Data Exploration}

The Enron dataset used in the project was created by the Udacity team. It was done by combining the Enron email and financial data. The data is stored in a python dictionary where each key-value pair in the dictionary corresponds to one person. For example \ref{pailou} shows the corresponding key-value data point of Enron executive Pai Lou, the only executive to reap and keep substantial wealth from Enron. Table \ref{poiTable} shows a list of persons of interest that were generated by the Udacity team.
\label{pailou}
\begin{verbatim}
data_dict['PAI LOU L']
{'bonus': 1000000,
 'deferral_payments': 'NaN',
 'deferred_income': 'NaN',
 'director_fees': 'NaN',
 'email_address': 'lou.pai@Enron.com',
 'exercised_stock_options': 15364167,
 'expenses': 32047,
 'fraction_from_poi': 0,
 'fraction_to_poi': 0,
 'from_messages': 'NaN',
 'from_poi_to_this_person': 'NaN',
 'from_this_person_to_poi': 'NaN',
 'loan_advances': 'NaN',
 'long_term_incentive': 'NaN',
 'other': 1829457,
 'poi': False,
 'restricted_stock': 8453763,
 'restricted_stock_deferred': 'NaN',
 'salary': 261879,
 'shared_receipt_with_poi': 'NaN',
 'to_messages': 'NaN',
 'total_payments': 3123383,
 'total_stock_value': 23817930}
\end{verbatim}


\begin{itemize}%
\item financial features:\\ \texttt{salary, deferral\_payments, total\_payments, loan\_advances, bonus, restricted\_stock\_deferred, deferred\_income, total\_stock\_value, expenses, exercised\_stock\_options, other, long\_term\_incentive, restricted\_stock, director\_fees}

\item email features:\\ \texttt{to\_messages, email\_address, from\_poi\_to\_this\_person, from\_messages, from\_this\_person\_to\_poi, shared\_receipt\_with\_poi}

\item POI label:\\ \texttt{poi}
\end{itemize}

\begin{table}[!htbp]
  \begin{center}
    \begin{tabular}{ |p{8cm}| } 
    \hline
    PERSONS OF INTEREST                \\ 

    \hline
    BELDEN TIMOTHY N    \\ 
    BOWEN JR RAYMOND M  \\ 
    CALGER CHRISTOPHER F\\ 
    CAUSEY RICHARD A    \\ 
    COLWELL WESLEY      \\ 
    DELAINEY DAVID W    \\ 
    FASTOW ANDREW S     \\ 
    GLISAN JR BEN F     \\ 
    HANNON KEVIN P      \\ 
    HIRKO JOSEPH        \\ 
    KOENIG MARK E       \\ 
    KOPPER MICHAEL J    \\ 
    LAY KENNETH L       \\ 
    RICE KENNETH D      \\ 
    RIEKER PAULA H      \\ 
    SHELBY REX          \\ 
    SKILLING JEFFREY K  \\ 
    YEAGER F SCOTT      \\ 
    \hline
    \end{tabular}
    \caption{Persons of interest in the dataset}
    \label{poiTable}
  \end{center}
\end{table}
While the original Enron dataset contained a robust amount of emails, this pared down one contains data for only about 150 people. Furthermore, the dataset is extremely skewed as can be seen from figure \ref{poi}.

\clearpage


\subsection*{Outlier detection}
 As part of the data exploration process, we were careful to analyze the data for potential outliers. One of the first tasked we did was visualize the salary of Enron executives \ref{salary}. From that visualization, it was clear that there was an outlier in the dataset. We found a data-point that was completely outside of a reasonable range as can be seen from figures \ref{salary} and \ref{SalaryBonus}. 

 A closer look revealed that the data-point was an input error which included the TOTAL of all salaries as its own row. The data-point was removed leaving the dataset in a more realistic state as can be seen from figure \ref{SalaryBonus2}.

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary}
    \label{salary}}
    
    \caption{\textbf{Plot of Enron employee salaries.}\textit{ From this visualization, we can see there is a huge spike to the right of data-point 60, this spike corresponds with an outlier of value \$26,704,229.00.}}
\end{figure}

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_bonus}
    \label{SalaryBonus}}
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_bonus2}
    \label{SalaryBonus2}}
    
    \caption{\textbf{TOTAL insertion.} \textit{Figure (a) Bonus versus salary with outlier present. Figure (b) Same dataset but without the outlier. By visualizing these two features in a scatter plot we were able to clearly detect the existence of an outlier. In addition, once the data has been cleaned of outliers, we can see that majority of data-points cluster in a close range, with a few data-points spread outside bonus range of \$2 million. From descriptive statistics, we learned that the 75\% of bonus amount was \$1 million. }}
\end{figure}




\section*{Exploratory Visualization}

One of the areas we concentrated on was that of compensation. We were very interested in the compensation packages as represented in the financial data of the executives. We moved ahead with a working hypothesis, that if fraud was indeed occurring at Enron, then more than likely, the money was probably going to be funneled out through paid bonuses and stocks. A more rigorous hypothesis would then correlate stock options granted and exercised by the Enron executives with the sales of the shares on the open market. However, such an investigation, is outside the scope of this project. 

\begin{table}[!htbp]
  \begin{center}
    \begin{tabular}{ |l|l|l| } 
    \hline
    NAME                &  SALARY        &  BONUS       \\ 

    \hline
    ALLEN PHILLIP K     &  \$201,955.00    &     4,175,000\\ 
    BELDEN TIMOTHY N    &  \$213,999.00    &     5,249,999\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &     5,600,000\\ 
    LAY KENNETH L       &  \$1,072,321.00  &     7,000,000\\ 
    \hilight{LAVORATO JOHN J}     &  \$339,288.00    &     8,000,000\\ 
    \hline
    \multicolumn{3}{|c|}{}\\
    \hline
    NAME                &  SALARY        &  EXERCISED STOCK OPTIONS \\

    \hline
    FREVERT MARK A      &  \$1,060,932.00  &    10,433,518\\ 
    \hilight{PAI LOU L}           &  \$261,879.00    &    15,364,167\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &    19,250,000\\ 
    RICE KENNETH D      &  \$420,636.00    &    19,794,175\\ 
    LAY KENNETH L       &  \$1,072,321.00  &    34,348,384\\ 
    \hline
    \multicolumn{3}{|c|}{}\\
    \hline
    NAME                &  SALARY        &  RESTRICTED STOCK\\ 

    \hline
    YEAGER F SCOTT      &  \$158,403.00    &     3,576,206\\ 
    IZZO LAWRENCE L     &  \$85,274.00     &     3,654,808\\ 
    BAXTER JOHN C       &  \$267,102.00    &     3,942,714\\ 
    KEAN STEVEN J       &  \$404,338.00    &     4,131,594\\ 
    FREVERT MARK A      &  \$1,060,932.00  &     4,188,667\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &     6,843,672\\ 
    \hilight{PAI LOU L}          &  \$261,879.00    &     8,453,763\\ 
    WHITE JR THOMAS E   &  \$317,543.00    &    13,847,074\\ 
    LAY KENNETH L       &  \$1,072,321.00  &    14,761,694\\ 
    \hline
    \end{tabular}
    \caption{Financial data on some of Enron's top paid employees.}
    \label{exStTable}
  \end{center}
\end{table}

To zoom in on the compensation, we focused on three features `bonus,', `exercised\_stock\_options' and `restricted\_stock'. Table \ref{exStTable} shows some financial data for some of the highest compensated employees. It comes as no surprise that we see that the sitting CEO at the time of the collapsce, Ken Lay, had the second highest bonus paid, that wasn't surprising. What was surprising was the name ``John Lavorato.'' He was the employee that got the highest bonus. So who was John Lavorato? He was the former head of Enron's trading operations. 

Another interesting character that showed up in the financial data was ``Pai Lou.'' From table \ref{exStTable}, one can see that he had some of the largest exercised stock options. Apart from the past CEOs and chairmen, he was \textit{the} employee that sold the most Enron stock. He also got some of the largest shares of restricted stock. Its interesting that these men aren't on the POI list in table \ref{poiTable}. I firmly believe they are probably on the co-conspirators unindicted list.

To gain insights into the relationship between salary and exercised stock options, we took that subset of data and ran a KMeans clustering algorithm on it to see how the data clustered; we focused on three clustered as can be seen from figure \ref{salaryExercisedStock}. The clusters generated matched our intuition about the executives, mostly all the five executives with the highest exercised stock options were clustered as one, as can be seen in the visualization in figure \ref{salaryExercisedStock} by the markers colored blue. It is important to note we were not able to infer a relationship between salary and exercised stock options as we had thought. 

We proceeded on, with a hypothesis that more than likely, the executives with the highest salaries, probably also had the highest restricted stock. Just as with exercised stock options, it wasn't the case. From figure \ref{salaryRestrictedStock} one can see that there is a data-point with a salary close to \$200,000, but with over 8 million shares, when most of the other folks around that salary bracket had less than a million shares. Its important to note that for both our visualizations, the KMeans clustering algorithm, clustered the data-points along both the exercised stock options and restricted stocks. 
\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_exercised_stock_options}
    \label{salaryExercisedStock}}
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/exercised_stock_options}
    \label{salaryExercisedStock2}}
    
    \caption{\textbf{A closer look at salary and exercised stock options.} \textit{Figure (a) A scatter plot of salary versus exercised stock options. Figure (b) A plot of exercised stock options. From these two plots one can see that there is a small subset of senior executives who exercised a significant amount of Enron shares. }}
\end{figure}

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_restricted_stock}
    \label{salaryRestrictedStock}}
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/restricted_stock}
    \label{salaryRestrictedStock2}}
    
    \caption{\textbf{A closer look at salary and restricted stock.} \textit{A scatter plot of salary versus restricted stock. Figure (b) A plot of restricted stock.}}
\end{figure}

\clearpage
\section*{Algorithms and Techniques}

For the problem of identifying fraud from the Enron financial and email data, we experimented with three different classifiers:
\begin{enumerate}% 
\item A RandomForestClassifier\\
We selected this learner because it is considered one of the best off-the-shelf learning algorithm, and requires almost no tuning. 
\item An ExtraTreesClassifier\\
 Its another ensemble learner like Random Forest, with one caveat. When creating a branch in a tree, Random Forest chooses the most discriminant value, whereas ExtrRandTrees split point is arbitrarily. This helps to increase the bias slightly and lower the variance even more.
\item A LogisticRegression (Robust, with $Lp$ = $L1$) \\
Its computational simpler than the ensemble methods. Furthermore, it has been used in real life to predict adverse risk events that have relatively small chances of occurring like credit card fraud and so on. The dataset composition of those events are similar to the Enron dataset, where the labels are heavily skewed towards one class.
We selected the robust LogisticRegression based on the fact that it handles outliers better. While the data has been cleaned of ``outliers,'' there are a few executives whose data-points fall outside of the mean by several standard deviations.
\end{enumerate}



\section*{Benchmark}

For this specific problem, we were unable to acquire benchmarks that we could test the performance of our learners against. However in the notes accompanying the project description, we were tasked to shoot for a precision score of over 0.3. From our three learners, and the initial trials, we have already superseded this number, as can be seen from table \ref{benchMarkScoresTable}.


%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------
\chapter*{Identify Fraud From Enron Data: Methodology}

\section*{Data Preprocessing}
Significant data preprocessing wasn't necessary for this problem. Once we ridded our dataset of outliers, we decided to play with a few features to get a feel of how they would behave with the algorithms. We tested out our three learners with salary and bonus features before we scaled them with a minimax-scaler and afterwards. We found no difference, so we decided to forgo scaling.

We suspected that there was interdependence of features so we engineered a feature, the ``fraction of emails to and from poi.'' We figured this metric could give us another window into discriminating the data. 

Our approach to feature selection was based on intuition as well as running the features through feature selection algorithms to guide us in our decision of features to use. We decided to use a RandomForest classifier to implement feature selection.

For our implementation, we used a baseline classifier, we fitted it to our data and used its ``\texttt{feature\_importances\_}'' to obtain a score on the strength of that feature. We ran this algorithm a 1000 and averaged the scores of the features to obtain the top $n$ features. The output of the experiment is as follows:
\begin{verbatim}
exercised_stock_options        944
total_stock_value              903
bonus                          886
expenses                       877
other                          875
total_payments                 709
restricted_stock               684
deferred_income                675
shared_receipt_with_poi        674
salary                         633
\end{verbatim}

\setlength{\extrarowheight}{1.5pt}
\begin{table}[!htbp]
\caption{Scores} %title of the table
\centering % centering table
\begin{tabular}{|p{6cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} % creating four columns
\hline % inserts single-line
\multicolumn{4}{|c|}{}\\
\multicolumn{4}{|c|}{Result of training with Features Selected by RandomForest Algorithm}\\[5pt]
\hline 
& \multicolumn{3}{c|}{Metrics}\\[5pt]
\cline{2-4} 
& Precision & Recall & F1\\[0.5ex]
\hline % inserts single-line

RandomForestClassifier      &  0.36602&  0.13250&  0.19457\\ 
ExtraTreesClassifier        &  0.37158&  0.14250&  0.20600\\ 
LogisticRegression          &  0.41523&  0.16900&  0.24023\\ 

\hline% inserts single-line
\multicolumn{4}{|c|}{}\\
\multicolumn{4}{|c|}{Result of training with ad-hoc Features based on domain knowledge}\\[5pt]
\hline 
& \multicolumn{3}{c|}{Metrics}\\[5pt]
\cline{2-4} 
& Precision & Recall & F1\\[0.5ex]
\hline % inserts single-line

RandomForestClassifier      &  0.39824&  0.13600&  0.20276\\ 
ExtraTreesClassifier        &  0.41558&  0.14400&  0.21389\\ 
LogisticRegression          &  0.49852&  0.16850&  0.25187\\ 

\hline % inserts single-line

\end{tabular}
\label{benchMarkScoresTable}
\end{table}

We we used this features on our three baseline learners, the results we got didn't necessarily improve over our ad-hoc, knowledge of domain intuition guided features, as can be seen from table \ref{benchMarkScoresTable}.

For the final algorithm, we ended up going with our intuition and selecting features that we had explored in the data analysis. These features are as follows: 
\begin{verbatim}
poi
bonus
exercised_stock_options
restricted_stock
fraction_from_poi
fraction_to_poi
from_poi_to_this_person
from_this_person_to_poi
salary
\end{verbatim}


\section*{Implementation}

We implemented these three learning algorithms. For each of the learners we implemented the baseline algorithm using 10 fold cross validation to get an accuracy score. These scores proved to be misleading. We went ahead and used a stratified shuffle split cross validation and calculated the precision, recall and $F_1$ scores respectively.

From our trials we generated the following scores as can be seen in table \ref{benchMarkScoresTable}. From these trials, it was evident that the robust Logistic regression beat out both the ensemble learners on all three metrics. Furthermore, the regression also had the least computational time of 0.026s, whereas the Random Forest and the ExtraTrees both computed in 0.236s.


\section*{Refinement \& Model Evaluation and Validation}
To improve on the performance of the algorithm, we decided to reduce the number of features that were used for training. Through some trial and error, we realized if we held the feature list to the first three features, our performance improve dramatically as can be seen in table \ref{reducedFeaturesTable}.
\begin{verbatim}
bonus
exercised_stock_options
restricted_stock 
\end{verbatim}

\setlength{\extrarowheight}{1.5pt}
\begin{table}[!htbp]
\caption{Result of training with reduced features} %title of the table
\centering % centering table
\begin{tabular}{|p{6cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|} % creating four columns
\hline % inserts single-line
& \multicolumn{3}{c|}{Metrics}\\[5pt]
\cline{2-4} 
& Precision & Recall & F1\\[0.5ex]
\hline % inserts single-line

RandomForestClassifier      &  0.49292  &0.19150 &0.27584\\ 
ExtraTreesClassifier        &  0.50255  &0.19700 &0.28305\\ 
LogisticRegression          &  0.63027  &0.20200 &0.30594\\ 
\hline% inserts single-line
\multicolumn{4}{|c|}{Results of tuning}\\[5pt]
\hline

LogisticRegression          &  0.73010  &0.18800  &0.29901\\ 
\hline
\end{tabular}
\label{reducedFeaturesTable}
\end{table}

Once we achieved this result, we moved forward with tuning the hyper-parameters of the robust logistic regression. We tuned the $C$ constant as well as the $tolerance$ through scikit's grid search using a stratified shuffle split with a 1000 folds. The algorithm selected the best parameters as follows: C=0.125, tol=0.0001. We fitted this new classifier and achieved a precision score of 0.73.
%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Identify Fraud From Enron Data: Conclusion}

\section*{Free-Form Visualization}
In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you visualized a relevant or important quality about the problem, dataset, input data, or results?
\item Is the visualization thoroughly analyzed and discussed?
\item If a plot is provided, are the axes, title, and datum clearly defined?
\end{itemize}


\section*{Reflection}
In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you thoroughly summarized the entire process you used for this project?
\item Were there any interesting aspects of the project?
\item Were there any difficult aspects of the project?
\item Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?
\end{itemize}

\section*{Improvement}
In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are there further improvements that could be made on the algorithms or techniques you used in this project?
\item Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?
\item If you used your final solution as the new benchmark, do you think an even better solution exists?
\end{itemize}







\end{document}




