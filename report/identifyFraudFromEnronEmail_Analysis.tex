\documentclass[twoside,openright,titlepage,numbers=noenddot,headinclude,%
               footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,%
               paper=a4,fontsize=11pt,ngerman,american]{scrreprt}

% Custom config ===============================================================

% Classic thesis
\usepackage{amssymb}
\input{classicthesis-config}

% Theorems and definitions
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}

\newtheorem{algorithm}{Algorithm}
\usepackage{algpseudocode}

% Counters
\renewcommand{\labelenumi}{{\color{halfgray}(\alph{enumi})}}
\renewcommand{\labelenumii}{\color{halfgray}{\roman{enumii}.}}
\renewcommand{\labelitemi}{{\color{halfgray}-}}%\raisebox{0.3ex}{\tiny$\blacksquare$}}}

\numberwithin{theorem}{chapter}
\numberwithin{definition}{chapter}
\numberwithin{algorithm}{chapter}
\numberwithin{figure}{chapter}
\numberwithin{table}{chapter}

% Maths
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\numberwithin{equation}{chapter}
\allowdisplaybreaks

% Shaded boxes
\usepackage{framed}
\newenvironment{remark}[1]{%
  \definecolor{shadecolor}{gray}{0.9}%
  \begin{shaded}{\color{Maroon}\noindent\textsc{#1}}\\%
}{%
  \end{shaded}%
}



% Code snippets
\usepackage{color}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}


% Todo
\newcommand{\todo}[1]{\textcolor{red}{[TODO] #1}}

% PS pictures
%\usepackage{pstricks,auto-pst-pdf}

% Landscape tables
\usepackage{rotating}

% Checkmarks
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Wide tables
\usepackage{ltablex}

%highlighting
\usepackage{color}
\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}


% -----------------------------------------------------------------------------

\begin{document}
\frenchspacing
\raggedbottom
\selectlanguage{american}
\pagenumbering{roman}
\pagestyle{plain}

% Front pages =================================================================

\include{titlepage}

% Content =====================================================================

\pagenumbering{arabic}

\cleardoublepage


\chapter*{Identify Fraud From Enron Email}

%----------------------------------------------------------------------------------------
%  CHAPTER CONTENTS
%----------------------------------------------------------------------------------------


\section*{Project Overview}

In early 2000s, the Enron corporation of Houston, Texas was considered one of the most profitable energy companies in the United States. Fortune magazine named Enron one of the most innovative companies in the US for six years in a row, 1996 - 2001. However, by late 2001, the Enron corporation filed for bankruptcy after it was revealed that the company overstated its profits and defrauded it share holders of a considerable amount of wealth. Several person were later indicted and convicted of fraud in the Enron case. 

The goal of this project is to use applied machine learning, based on released Enron data---Enron emails and financial accounts---to identify persons of interest in the Enron debacle.


\section*{Problem Statement}

The problem that we are interested in solving is building an algorithm that can help us identify persons who might be of interest with regards to fraud at Enron. These persons are predominantly Enron employees and consultants who worked for the corporation. 

The dataset that we have available for this task was collected over two weeks in 2002 by the Federal Energy Regulatory Commission (FERC) during its investigation into the case. The dataset contains over 600,000 emails of 158 high-level Enron employees as well as the financial records of stock payments, salary and so forth of those employees.

To identify persons of interest that might have been party to fraud at Enron we suggest the following strategy:
\begin{enumerate}% 
\item Explore the dataset to ensure its integrity and understand the context.
\item Identify features that may be used. If possible, engineer features that might provide greater discrimination.
\item With the understanding that this a ``classification'' task, explore a couple of classifiers that might be well suited for the problem at hand.
\item Once classifiers have been identified, tune them for optimality.
\end{enumerate}



\section*{Metrics}

The Enron dataset while it contains a robust amount of emails, contains data for only about 150 people. As such, 

In this section, you will need to clearly define the metrics or calculations you will use to measure performance of a model or result in your project. These calculations and metrics should be justified based on the characteristics of the problem and problem domain. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are the metrics you've chosen to measure the performance of your models clearly discussed and defined?
\item Have you provided reasonable justification for the metrics chosen based on the problem and solution?
\end{itemize}

accuracy, precision, recall, f1, f2
true positives, false positives, false negatives, true negatives)

%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Analysis}

\section*{Data Exploration}

The Enron dataset used in the project was created by the Udacity team. It was done by combining the Enron email and financial data. The data is stored in a python dictionary where each key-value pair in the dictionary corresponds to one person. For example \ref{pailou} shows the corresponding key-value data point of Enron executive Pai Lou, the only executive to reap and keep substantial wealth from Enron. Table \ref{poiTable} shows a list of persons of interest that were generated by the Udacity team.
\label{pailou}
\begin{verbatim}
data_dict['PAI LOU L']
{'bonus': 1000000,
 'deferral_payments': 'NaN',
 'deferred_income': 'NaN',
 'director_fees': 'NaN',
 'email_address': 'lou.pai@Enron.com',
 'exercised_stock_options': 15364167,
 'expenses': 32047,
 'fraction_from_poi': 0,
 'fraction_to_poi': 0,
 'from_messages': 'NaN',
 'from_poi_to_this_person': 'NaN',
 'from_this_person_to_poi': 'NaN',
 'loan_advances': 'NaN',
 'long_term_incentive': 'NaN',
 'other': 1829457,
 'poi': False,
 'restricted_stock': 8453763,
 'restricted_stock_deferred': 'NaN',
 'salary': 261879,
 'shared_receipt_with_poi': 'NaN',
 'to_messages': 'NaN',
 'total_payments': 3123383,
 'total_stock_value': 23817930}
\end{verbatim}


\begin{itemize}%
\item financial features:\\ \texttt{salary, deferral\_payments, total\_payments, loan\_advances, bonus, restricted\_stock\_deferred, deferred\_income, total\_stock\_value, expenses, exercised\_stock\_options, other, long\_term\_incentive, restricted\_stock, director\_fees}

\item email features:\\ \texttt{to\_messages, email\_address, from\_poi\_to\_this\_person, from\_messages, from\_this\_person\_to\_poi, shared\_receipt\_with\_poi}

\item POI label:\\ \texttt{poi}
\end{itemize}

\begin{table}[!htbp]
  \begin{center}
    \begin{tabular}{ |p{8cm}| } 
    \hline
    PERSONS OF INTEREST                \\ 

    \hline
    BELDEN TIMOTHY N    \\ 
    BOWEN JR RAYMOND M  \\ 
    CALGER CHRISTOPHER F\\ 
    CAUSEY RICHARD A    \\ 
    COLWELL WESLEY      \\ 
    DELAINEY DAVID W    \\ 
    FASTOW ANDREW S     \\ 
    GLISAN JR BEN F     \\ 
    HANNON KEVIN P      \\ 
    HIRKO JOSEPH        \\ 
    KOENIG MARK E       \\ 
    KOPPER MICHAEL J    \\ 
    LAY KENNETH L       \\ 
    RICE KENNETH D      \\ 
    RIEKER PAULA H      \\ 
    SHELBY REX          \\ 
    SKILLING JEFFREY K  \\ 
    YEAGER F SCOTT      \\ 
    \hline
    \end{tabular}
    \caption{Persons of interest in the dataset}
    \label{poiTable}
  \end{center}
\end{table}
While the original Enron dataset contained a robust amount of emails, this pared down one contains data for only about 150 people. Furthermore, the dataset is extremely skewed as can be seen from figure \ref{poi}.

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/poi}
    \label{poi}}
    
    \caption{\textbf{Person of Interest plot.}\textit{ By visualizing this feature which acts as the label for our classifiers, we can see that the dataset is extremely skewed in favor of non-persons of interest.}}
\end{figure}

\clearpage


\subsection*{Outlier detection}
 As part of the data exploration process, we were careful to analyze the data for potential outliers. One of the first tasked we did was visualize the salary of Enron executives \ref{salary}. From that visualization, it was clear that there was an outlier in the dataset. We found a datapoint that was completely outside of a reasonable range as can be seen from figures \ref{salary} and \ref{SalaryBonus}. 

 A closer look revealed that the datapoint was an input error which included the TOTAL of all salaries as its own row. The datapoint was removed leaving the dataset in a more realistic state as can be seen from figure \ref{SalaryBonus2}.

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary}
    \label{salary}}
    
    \caption{\textbf{Plot of Enron employee sataries.}\textit{ From this visualization, we can see there is a huge spike to the right of datapoint 60, this spike corresponds with an outlier of value \$26,704,229.00.}}
\end{figure}

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_bonus}
    \label{SalaryBonus}}
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_bonus2}
    \label{SalaryBonus2}}
    
    \caption{\textbf{TOTAL insertion.} \textit{Figure (a) Bonus versus salary with outlier present. Figure (b) Same dataset but without the outlier. By visualizing these two features in a scatter plot we were able to clearly detect the existence of an outlier.}}
\end{figure}




\section*{Exploratory Visualization}

One of the areas we concentrated on was that of compensation. We were very interested in the compensation packages as represented in the financial data of the executives. We moved ahead with a working hypothesis, that if fraud was indeed occurring at Enron, then more than likely, the money was probably going to be funneled out through paid bonuses and stocks. A more rigorous hypothesis would then correlate stock options granted and exercised by the Enron executives with the sales of the shares on the open market. However, such an investigation, is outside the scope of this project. 

\begin{table}[!htbp]
  \begin{center}
    \begin{tabular}{ |l|l|l| } 
    \hline
    NAME                &  SALARY        &  BONUS       \\ 

    \hline
    ALLEN PHILLIP K     &  \$201,955.00    &     4,175,000\\ 
    BELDEN TIMOTHY N    &  \$213,999.00    &     5,249,999\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &     5,600,000\\ 
    LAY KENNETH L       &  \$1,072,321.00  &     7,000,000\\ 
    \hilight{LAVORATO JOHN J}     &  \$339,288.00    &     8,000,000\\ 
    \hline
    \multicolumn{3}{|c|}{}\\
    \hline
    NAME                &  SALARY        &  EXERCISED STOCK OPTIONS \\

    \hline
    FREVERT MARK A      &  \$1,060,932.00  &    10,433,518\\ 
    \hilight{PAI LOU L}           &  \$261,879.00    &    15,364,167\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &    19,250,000\\ 
    RICE KENNETH D      &  \$420,636.00    &    19,794,175\\ 
    LAY KENNETH L       &  \$1,072,321.00  &    34,348,384\\ 
    \hline
    \multicolumn{3}{|c|}{}\\
    \hline
    NAME                &  SALARY        &  RESTRICTED STOCK\\ 

    \hline
    YEAGER F SCOTT      &  \$158,403.00    &     3,576,206\\ 
    IZZO LAWRENCE L     &  \$85,274.00     &     3,654,808\\ 
    BAXTER JOHN C       &  \$267,102.00    &     3,942,714\\ 
    KEAN STEVEN J       &  \$404,338.00    &     4,131,594\\ 
    FREVERT MARK A      &  \$1,060,932.00  &     4,188,667\\ 
    SKILLING JEFFREY K  &  \$1,111,258.00  &     6,843,672\\ 
    \hilight{PAI LOU L}          &  \$261,879.00    &     8,453,763\\ 
    WHITE JR THOMAS E   &  \$317,543.00    &    13,847,074\\ 
    LAY KENNETH L       &  \$1,072,321.00  &    14,761,694\\ 
    \hline
    \end{tabular}
    \caption{Financial data on some of Enron's top paid employees.}
    \label{exStTable}
  \end{center}
\end{table}

To zoom in on the compensation, we focused on three features `bonus,', `exercised\_stock\_options' and `restricted\_stock'. Table \ref{exStTable} shows some financial data for some of the highest compensated employees. It comes as no surprise that we see that the sitting CEO at the time of the collapsce, Ken Lay, had the second highest bonus paid, that wasn't surprising. What was surprising was the name ``John Lavorato.'' He was the employee that got the highest bonus. So who was John Lavorato? He was the former head of Enron's trading operations. 

Another interesting character that showed up in the financial data was ``Pai Lou.'' From table \ref{exStTable}, one can see that he had some of the largest exercised stock options. Apart from the past CEOs and chairmen, he was \textit{the} employee that sold the most Enron stock. He also got some of the largest shares of restricted stock. Its interesting that these men aren't on the POI list in table \ref{poiTable}. I firmly believe they are probably on the co-conspirators unindicted list.


\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/salary_v_exercised_stock_options}
    \label{salaryExercisedStock}}
    \subfloat[]{%
    \includegraphics[width=0.6\textwidth]{figures/exercised_stock_options}
    \label{salaryExercisedStock2}}
    
    \caption{\textbf{A closer look at salary and exercised stock options.} \textit{Figure (a) A scatter plot of salary versus exercised stock options. Figure (b) A plot of exercised stock options. From these two plots one can see that for most of the senior executives, .}}
\end{figure}

\begin{figure}[!hbtp]
\centering
    \subfloat[]{%
    \includegraphics[width=0.5\textwidth]{figures/salary_v_restricted_stock}
    \label{salaryRestrictedStock}}
    \subfloat[]{%
    \includegraphics[width=0.5\textwidth]{figures/restricted_stock}
    \label{salaryRestrictedStock2}}
    
    \caption{\textbf{TOTAL insertion.} \textit{Figure (a) Bonus versus salary with outlier present. Figure (b) Same dataset but without the outlier. By visualizing these two features in a scatter plot we were able to clearly detect the existence of an outlier.}}
\end{figure}


\section*{Algorithms and Techniques}

In this section, you will need to discuss the algorithms and techniques you intend to use for solving the problem. You should justify the use of each one based on the characteristics of the problem and the problem domain. 

What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: ``pick an algorithm'']

Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are the algorithms you will use, including any default variables/parameters in the project clearly defined?
\item Are the techniques to be used thoroughly discussed and justified?
\item Is it made clear how the input data or datasets will be handled by the algorithms and techniques chosen?
\end{itemize}

\section*{Benchmark}

In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Has some result or value been provided that acts as a benchmark for measuring performance?
\item Is it clear how this result or value was obtained (whether by data or by hypothesis)?
\end{itemize}

%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------
\chapter*{Methodology}

\section*{Data Preprocessing}
In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. 

What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) 

In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: ``create new features'', ``properly scale features'', ``intelligently select feature'']

Questions to ask yourself when writing this section:
\begin{itemize}% 
\item If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?
\item Based on the Data Exploration section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?
\item If no preprocessing is needed, has it been made clear why?
\end{itemize}


\section*{Implementation}
In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?
\item Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?
\item Was there any part of the coding process (e.g., writing complicated functions) that should be documented?
\end{itemize}


\section*{Refinement}
In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. 

What does it mean to tune the parameters of an algorithm, and what can happen if you don't do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: ``tune the algorithm'']

Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Has an initial solution been found and clearly reported?
\item Is the process of improvement clearly documented, such as what techniques were used?
\item Are intermediate and final solutions clearly reported as the process is improved?
\end{itemize}
%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Results}


\section*{Model Evaluation and Validation}
In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). 


Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?
\item Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?
\item Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?
\item Can results found from the model be trusted?
\end{itemize}

\section*{Justification}
In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are the final results found stronger than the benchmark result reported earlier?
\item Have you thoroughly analyzed and discussed the final solution?
\item Is the final solution significant enough to have solved the problem?
\end{itemize}

%----------------------------------------------------------------------------------------
%  CHAPTER 
%----------------------------------------------------------------------------------------

\chapter*{Conclusion}

\section*{Free-Form Visualization}
In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you visualized a relevant or important quality about the problem, dataset, input data, or results?
\item Is the visualization thoroughly analyzed and discussed?
\item If a plot is provided, are the axes, title, and datum clearly defined?
\end{itemize}


\section*{Reflection}
In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Have you thoroughly summarized the entire process you used for this project?
\item Were there any interesting aspects of the project?
\item Were there any difficult aspects of the project?
\item Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?
\end{itemize}

\section*{Improvement}
In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:
\begin{itemize}% 
\item Are there further improvements that could be made on the algorithms or techniques you used in this project?
\item Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?
\item If you used your final solution as the new benchmark, do you think an even better solution exists?
\end{itemize}







\end{document}




